{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFVY74xtw2gT",
        "outputId": "7b9ceec4-09bf-48a4-ac3c-fa008845fe20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "\n",
        "# initialize our model to support 3 classes (Positive, Negative and Neutral)\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do9btnq6xYs0",
        "outputId": "9fe98de6-a436-44c2-b7f8-782a13d0d454"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "#!pip uninstall numpy -y\n",
        "#!pip install --upgrade pip\n",
        "#!pip install numpy\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "AAm73NGBxj_D"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Loading data\n",
        "data = pd.read_csv('sentiment_data.csv')\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "data['Sentiment'] = label_encoder.fit_transform(data['Sentiment'])\n",
        "\n",
        "print(\"Total entries: \", data.shape[0])\n",
        "\n",
        "# 0 - Negative | 1 - Neutral | 2 - Positive\n",
        "print(data['Sentiment'].value_counts())"
      ],
      "metadata": {
        "id": "eV1e7R6g306s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1223e4e-6060-4914-d20c-c2d3ea5057da"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total entries:  5842\n",
            "1    3130\n",
            "2    1852\n",
            "0     860\n",
            "Name: Sentiment, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.20)"
      ],
      "metadata": {
        "id": "4WIg0vgT4BMd"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating tokens of each sentence in sentences\n",
        "def tokenize_sentences(sentences):\n",
        "    input_ids, token_type_ids, attention_masks = [], [], []\n",
        "    for sentence in sentences:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        token_type_ids.append(encoded['token_type_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    return input_ids, token_type_ids, attention_masks\n",
        "\n",
        "example=\"My name is Pulkit Agarwal\"\n",
        "input_ids, token_type_ids, attention_masks = tokenize_sentences([example])\n",
        "print(\"input_ids: \", input_ids)\n",
        "print(\"token_type_ids: \", token_type_ids)\n",
        "print(\"attention_masks: \", attention_masks)"
      ],
      "metadata": {
        "id": "gOu1l9u04S47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e50315-66d0-4001-dfa9-fe2a3e4cf961"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids:  [[101, 2026, 2171, 2003, 16405, 13687, 4183, 12943, 2906, 13476, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "token_type_ids:  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "attention_masks:  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_create_dataset(data, batch_size):\n",
        "    input_ids, token_type_ids, attention_masks = tokenize_sentences(data['Sentence'])\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "          'input_ids': input_ids,\n",
        "          'token_type_ids': token_type_ids,\n",
        "          'attention_mask': attention_masks\n",
        "        },\n",
        "        data['Sentiment']\n",
        "    )).batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = tokenize_and_create_dataset(train_data, batch_size=32)\n",
        "val_dataset = tokenize_and_create_dataset(val_data, batch_size=32)"
      ],
      "metadata": {
        "id": "SCg4yPRK4ZKj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer = AdamWeightDecay(lr=3e-5)\n",
        "#loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "#metrics = ['accuracy']\n",
        "#model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "# CP for accuracy\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])"
      ],
      "metadata": {
        "id": "Jm4SOqkn4uCl"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using epoc = 1 because of time xD\n",
        "history = model.fit(train_dataset, validation_data=val_dataset)"
      ],
      "metadata": {
        "id": "L9ePMlu14vZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06d6076-1785-4355-9349-deac51ea75f4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "147/147 [==============================] - 137s 930ms/step - loss: 0.3630 - accuracy: 0.8335 - val_loss: 0.4836 - val_accuracy: 0.7571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc = history.history['val_accuracy'][-1]\n",
        "\n",
        "print(\"Validation accuracy: \", val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKqrcyl9JrEm",
        "outputId": "a8456d8b-4d38-49bd-b482-30c6e16cb90e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy:  0.7570573091506958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sentence, model, tokenizer):\n",
        "    #reuse the token creation code for now\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=True,\n",
        "        truncation=True\n",
        "    )\n",
        "    input_ids = tf.expand_dims(encoded['input_ids'], 0)\n",
        "    token_type_ids = tf.expand_dims(encoded['token_type_ids'], 0)\n",
        "    attention_mask = tf.expand_dims(encoded['attention_mask'], 0)\n",
        "\n",
        "    logits = model.predict({\n",
        "        'input_ids': input_ids,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'attention_mask': attention_mask\n",
        "    })[0]\n",
        "    probabilities = tf.nn.softmax(logits)\n",
        "    prediction = tf.argmax(probabilities, axis=-1).numpy()[0]\n",
        "    return prediction\n",
        "\n",
        "sentence = val_data['Sentence'].iloc[1]\n",
        "prediction = predict(sentence, model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omVxxMx3Dj8Z",
        "outputId": "83324858-1e85-44f7-9370-1e1df23d2d19"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predictions: 0 -> Negative | 1 -> Neutral | 2 -> Positive\")\n",
        "print(\"Predicted sentiment: \", prediction)\n",
        "actual_label = val_data['Sentiment'].iloc[1]\n",
        "print(\"Actual sentiment: \", label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oddUnuj7EdaG",
        "outputId": "25d7c925-7ae1-4669-8cba-867981d3c11f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: 0 -> Negative | 1 -> Neutral | 2 -> Positive\n",
            "Predicted sentiment:  1\n",
            "Actual sentiment:  negative\n"
          ]
        }
      ]
    }
  ]
}